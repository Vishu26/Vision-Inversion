{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de949ef8",
   "metadata": {},
   "source": [
    "# Image Inversion with Vision Models\n",
    "\n",
    "This notebook demonstrates image inversion using various vision models including DINOv3, DINOv2, and CLIP. Image inversion reconstructs an image from its feature representations extracted by a vision model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers pillow numpy matplotlib timm open-clip-torch ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, Dict\n",
    "import warnings\n",
    "import io\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import widgets for file upload\n",
    "try:\n",
    "    from IPython.display import display, clear_output\n",
    "    from ipywidgets import FileUpload, Button, HBox, VBox, Label, Output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"Note: ipywidgets not available. File upload will use alternative method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05579f",
   "metadata": {},
   "source": [
    "## Vision Model Wrapper Classes\n",
    "\n",
    "We'll create wrapper classes for different vision models to provide a unified interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6041ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionModelWrapper:\n",
    "    \"\"\"Base class for vision model wrappers\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.transform = None\n",
    "        self._setup_model()\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        \"\"\"Override in subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract features from an image tensor\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_image_size(self) -> int:\n",
    "        \"\"\"Return the expected input image size\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_normalize_transform(self):\n",
    "        \"\"\"Return normalization transform\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2Wrapper(VisionModelWrapper):\n",
    "    \"\"\"Wrapper for DINOv2 model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = None, use_keys: bool = False):\n",
    "        self.use_keys = use_keys\n",
    "        super().__init__(model_name, device)\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        from transformers import Dinov2Model, AutoImageProcessor\n",
    "        \n",
    "        # Use AutoImageProcessor for compatibility (though we do manual preprocessing)\n",
    "        self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        self.model = Dinov2Model.from_pretrained('facebook/dinov2-base').to(self.device)\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = False\n",
    "        self.model.eval()\n",
    "        self.image_size = 224\n",
    "        \n",
    "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        # Ensure image is in [0, 1] range and properly normalized\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # DINOv2 expects normalized images (ImageNet normalization)\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        image_normalized = normalize(image)\n",
    "        \n",
    "        if self.use_keys:\n",
    "            # Extract keys from the last attention layer\n",
    "            # Get embeddings first\n",
    "            embedding_output = self.model.embeddings(pixel_values=image_normalized)\n",
    "            \n",
    "            # Pass through encoder layers to get to the last layer\n",
    "            hidden_states = embedding_output\n",
    "            \n",
    "            # Pass through all layers except the last one\n",
    "            for layer in self.model.encoder.layer[:-1]:\n",
    "                layer_outputs = layer(hidden_states)\n",
    "                hidden_states = layer_outputs[0]\n",
    "            \n",
    "            # For the last layer, extract keys manually\n",
    "            last_layer = self.model.encoder.layer[-1]\n",
    "            attention_module = last_layer.attention.attention\n",
    "            \n",
    "            # Get the input to attention - check for layer norm in different locations\n",
    "            attn_input = hidden_states\n",
    "            if hasattr(last_layer, 'layernorm_before'):\n",
    "                attn_input = last_layer.layernorm_before(hidden_states)\n",
    "            elif hasattr(last_layer, 'layer_norm'):\n",
    "                attn_input = last_layer.layer_norm(hidden_states)\n",
    "            elif hasattr(last_layer.attention, 'layer_norm'):\n",
    "                attn_input = last_layer.attention.layer_norm(hidden_states)\n",
    "            \n",
    "            # Access key projection - try different possible attribute names\n",
    "            key_proj = None\n",
    "            if hasattr(attention_module, 'k_proj'):\n",
    "                key_proj = attention_module.k_proj\n",
    "            elif hasattr(attention_module, 'key'):\n",
    "                key_proj = attention_module.key\n",
    "            elif hasattr(attention_module, 'query_key_value'):\n",
    "                # Some models use combined QKV projection\n",
    "                qkv = attention_module.query_key_value(attn_input)\n",
    "                # Split into Q, K, V (assuming they're concatenated)\n",
    "                # This depends on the model structure\n",
    "                hidden_size = qkv.shape[-1]\n",
    "                head_dim = hidden_size // (3 * attention_module.num_attention_heads)\n",
    "                qkv = qkv.view(attn_input.shape[0], attn_input.shape[1], 3, attention_module.num_attention_heads, head_dim)\n",
    "                key_states = qkv[:, :, 1]  # Extract K from QKV [batch, tokens, heads, head_dim]\n",
    "                # Reshape to [batch, tokens, heads * head_dim]\n",
    "                batch_size, num_tokens = attn_input.shape[0], attn_input.shape[1]\n",
    "                key_states = key_states.permute(0, 2, 1, 3).contiguous()  # [batch, heads, tokens, head_dim]\n",
    "                key_states = key_states.view(batch_size, num_tokens, hidden_size // 3)\n",
    "                all_tokens = key_states\n",
    "                key_proj = None  # Already computed\n",
    "            \n",
    "            if key_proj is not None:\n",
    "                # Apply key projection\n",
    "                key_states = key_proj(attn_input)\n",
    "                \n",
    "                # Handle different shapes\n",
    "                if len(key_states.shape) == 2:\n",
    "                    # Shape: [num_tokens, hidden_dim] - missing batch dimension\n",
    "                    # Add batch dimension: [1, num_tokens, hidden_dim]\n",
    "                    key_states = key_states.unsqueeze(0)\n",
    "                    all_tokens = key_states\n",
    "                elif len(key_states.shape) == 3:\n",
    "                    # Already in [batch_size, num_tokens, hidden_dim] format\n",
    "                    all_tokens = key_states\n",
    "                elif len(key_states.shape) == 4:\n",
    "                    # Reshape from [batch_size, num_heads, num_tokens, head_dim] or [batch_size, num_tokens, num_heads, head_dim]\n",
    "                    batch_size = key_states.shape[0]\n",
    "                    if key_states.shape[1] < key_states.shape[2]:  # num_heads < num_tokens\n",
    "                        # [batch_size, num_heads, num_tokens, head_dim]\n",
    "                        num_heads, num_tokens, head_dim = key_states.shape[1], key_states.shape[2], key_states.shape[3]\n",
    "                        key_states = key_states.permute(0, 2, 1, 3)  # [batch_size, num_tokens, num_heads, head_dim]\n",
    "                    else:\n",
    "                        # [batch_size, num_tokens, num_heads, head_dim]\n",
    "                        num_tokens, num_heads, head_dim = key_states.shape[1], key_states.shape[2], key_states.shape[3]\n",
    "                    \n",
    "                    hidden_dim = num_heads * head_dim\n",
    "                    key_states = key_states.contiguous().view(batch_size, num_tokens, hidden_dim)\n",
    "                    all_tokens = key_states\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected key_states shape: {key_states.shape}. Expected 2D, 3D, or 4D tensor.\")\n",
    "            \n",
    "            # Ensure we have the right shape\n",
    "            if len(all_tokens.shape) != 3:\n",
    "                # Fallback: use hidden states\n",
    "                print(\"Warning: Keys extraction failed, using hidden states\")\n",
    "                outputs = self.model(pixel_values=image_normalized)\n",
    "                all_tokens = outputs.last_hidden_state\n",
    "            \n",
    "            # Clear intermediate variables to free memory\n",
    "            try:\n",
    "                del hidden_states, embedding_output, last_layer, attention_module\n",
    "                if 'attn_input' in locals():\n",
    "                    del attn_input\n",
    "                if 'key_states' in locals():\n",
    "                    del key_states\n",
    "            except:\n",
    "                pass\n",
    "            clear_cuda_cache()\n",
    "        else:\n",
    "            # Process through the model normally\n",
    "            outputs = self.model(pixel_values=image_normalized)\n",
    "            # Extract all tokens (including CLS token) - shape: [batch_size, num_tokens, hidden_dim]\n",
    "            all_tokens = outputs.last_hidden_state\n",
    "        \n",
    "        # Normalize each token using L2 normalization\n",
    "        # Normalize along the feature dimension (dim=2)\n",
    "        all_tokens_normalized = nn.functional.normalize(all_tokens, p=2, dim=2)\n",
    "        \n",
    "        # Flatten to [batch_size, num_tokens * hidden_dim] for easier loss computation\n",
    "        batch_size = all_tokens_normalized.shape[0]\n",
    "        all_tokens_flat = all_tokens_normalized.view(batch_size, -1)\n",
    "        \n",
    "        # Clear memory\n",
    "        del all_tokens, all_tokens_normalized\n",
    "        clear_cuda_cache()\n",
    "        \n",
    "        return all_tokens_flat\n",
    "    \n",
    "    def get_image_size(self) -> int:\n",
    "        return self.image_size\n",
    "    \n",
    "    def get_normalize_transform(self):\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec143f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv3Wrapper(VisionModelWrapper):\n",
    "    \"\"\"Wrapper for DINOv3 model\"\"\"\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        try:\n",
    "            # Try to load DINOv3 from transformers (if available)\n",
    "            from transformers import AutoModel, AutoImageProcessor\n",
    "            self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "            self.model = AutoModel.from_pretrained('facebook/dinov2-large').to(self.device)\n",
    "            self.model.eval()\n",
    "            self.image_size = 224\n",
    "        except:\n",
    "            # Fallback to DINOv2 large as DINOv3 might not be available\n",
    "            print(\"DINOv3 not found, using DINOv2-large instead\")\n",
    "            from transformers import Dinov2Model, AutoImageProcessor\n",
    "            self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "            self.model = Dinov2Model.from_pretrained('facebook/dinov2-large').to(self.device)\n",
    "            self.model.eval()\n",
    "            self.image_size = 224\n",
    "    \n",
    "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            # Ensure image is in [0, 1] range\n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "            \n",
    "            # Normalize for ImageNet\n",
    "            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            image_normalized = normalize(image)\n",
    "            \n",
    "            outputs = self.model(pixel_values=image_normalized)\n",
    "            # Extract all tokens (including CLS token) - shape: [batch_size, num_tokens, hidden_dim]\n",
    "            all_tokens = outputs.last_hidden_state\n",
    "            \n",
    "            # Normalize each token using L2 normalization\n",
    "            # Normalize along the feature dimension (dim=2)\n",
    "            all_tokens_normalized = nn.functional.normalize(all_tokens, p=2, dim=2)\n",
    "            \n",
    "            # Flatten to [batch_size, num_tokens * hidden_dim] for easier loss computation\n",
    "            batch_size = all_tokens_normalized.shape[0]\n",
    "            all_tokens_flat = all_tokens_normalized.view(batch_size, -1)\n",
    "            \n",
    "            return all_tokens_flat\n",
    "    \n",
    "    def get_image_size(self) -> int:\n",
    "        return self.image_size\n",
    "    \n",
    "    def get_normalize_transform(self):\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWrapper(VisionModelWrapper):\n",
    "    \"\"\"Wrapper for CLIP model\"\"\"\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        self.use_clip_lib = False\n",
    "        try:\n",
    "            import clip\n",
    "            self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "            self.model.eval()\n",
    "            self.image_size = 224\n",
    "            self.use_clip_lib = True\n",
    "        except:\n",
    "            # Fallback to transformers CLIP\n",
    "            print(\"Using transformers CLIP instead of clip library\")\n",
    "            from transformers import CLIPProcessor, CLIPModel\n",
    "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "            self.model.eval()\n",
    "            self.image_size = 224\n",
    "    \n",
    "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            # Ensure image is in [0, 1] range\n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "            \n",
    "            normalize = transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                std=[0.26862954, 0.26130258, 0.27577711]\n",
    "            )\n",
    "            image_normalized = normalize(image)\n",
    "            \n",
    "            if self.use_clip_lib:\n",
    "                # CLIP library - manually extract all tokens from vision transformer\n",
    "                try:\n",
    "                    # Patch embedding\n",
    "                    x = self.model.visual.conv1(image_normalized)  # [batch_size, hidden_dim, grid_size, grid_size]\n",
    "                    x = x.reshape(x.shape[0], x.shape[1], -1)  # [batch_size, hidden_dim, grid_size^2]\n",
    "                    x = x.permute(0, 2, 1)  # [batch_size, grid_size^2, hidden_dim]\n",
    "                    \n",
    "                    # Add CLS token\n",
    "                    class_embedding = self.model.visual.class_embedding.expand(x.shape[0], 1, -1)\n",
    "                    x = torch.cat([class_embedding, x], dim=1)  # [batch_size, num_tokens, hidden_dim]\n",
    "                    \n",
    "                    # Add positional embeddings\n",
    "                    x = x + self.model.visual.positional_embedding\n",
    "                    \n",
    "                    # Pass through transformer\n",
    "                    x = self.model.visual.ln_pre(x)\n",
    "                    x = x.permute(1, 0, 2)  # [num_tokens, batch_size, hidden_dim] for transformer\n",
    "                    x = self.model.visual.transformer(x)\n",
    "                    x = x.permute(1, 0, 2)  # [batch_size, num_tokens, hidden_dim]\n",
    "                    \n",
    "                    all_tokens = x\n",
    "                except Exception as e:\n",
    "                    # Fallback: use pooled features and expand to simulate tokens\n",
    "                    print(f\"Warning: Could not extract token features from CLIP library, using pooled features: {e}\")\n",
    "                    pooled = self.model.encode_image(image_normalized)\n",
    "                    # Expand pooled features to simulate tokens (less ideal but works)\n",
    "                    all_tokens = pooled.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "            else:\n",
    "                # Transformers CLIP - access vision model to get token features\n",
    "                vision_outputs = self.model.vision_model(pixel_values=image_normalized)\n",
    "                # Extract all tokens from last hidden state\n",
    "                all_tokens = vision_outputs.last_hidden_state\n",
    "            \n",
    "            # Normalize each token using L2 normalization\n",
    "            # Normalize along the feature dimension (dim=2)\n",
    "            all_tokens_normalized = nn.functional.normalize(all_tokens, p=2, dim=2)\n",
    "            \n",
    "            # Flatten to [batch_size, num_tokens * hidden_dim] for easier loss computation\n",
    "            batch_size = all_tokens_normalized.shape[0]\n",
    "            all_tokens_flat = all_tokens_normalized.view(batch_size, -1)\n",
    "            \n",
    "            return all_tokens_flat\n",
    "    \n",
    "    def get_image_size(self) -> int:\n",
    "        return self.image_size\n",
    "    \n",
    "    def get_normalize_transform(self):\n",
    "        return transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                                    std=[0.26862954, 0.26130258, 0.27577711])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc74e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vision_model(model_type: str, device: str = None, use_keys: bool = False) -> VisionModelWrapper:\n",
    "    \"\"\"Factory function to get vision model wrapper\n",
    "    \n",
    "    Args:\n",
    "        model_type: Type of vision model ('dinov2', 'dinov3', 'clip')\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "        use_keys: For DINOv2, whether to extract keys from last attention layer instead of hidden states\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    model_type = model_type.lower()\n",
    "    if model_type in ['dinov2', 'dino-v2']:\n",
    "        return DINOv2Wrapper('dinov2', device, use_keys=use_keys)\n",
    "    elif model_type in ['dinov3', 'dino-v3']:\n",
    "        return DINOv3Wrapper('dinov3', device)\n",
    "    elif model_type == 'clip':\n",
    "        return CLIPWrapper('clip', device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}. Choose from: dinov2, dinov3, clip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d03815",
   "metadata": {},
   "source": [
    "## Image Inversion Class\n",
    "\n",
    "This class performs the actual image inversion using gradient-based optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInverter:\n",
    "    \"\"\"Performs image inversion from vision model features\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_model: VisionModelWrapper, \n",
    "                 num_iterations: int = 1000,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 use_tv_loss: bool = True,\n",
    "                 tv_weight: float = 0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vision_model: Vision model wrapper instance\n",
    "            num_iterations: Number of optimization iterations\n",
    "            learning_rate: Learning rate for optimization\n",
    "            use_tv_loss: Whether to use total variation loss for regularization\n",
    "            tv_weight: Weight for total variation loss\n",
    "        \"\"\"\n",
    "        self.vision_model = vision_model\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_tv_loss = use_tv_loss\n",
    "        self.tv_weight = tv_weight\n",
    "        self.device = vision_model.device\n",
    "        \n",
    "    def total_variation_loss(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Calculate total variation loss for regularization\"\"\"\n",
    "        batch_size = img.size()[0]\n",
    "        h_x = img.size()[2]\n",
    "        w_x = img.size()[3]\n",
    "        count_h = self._tensor_size(img[:, :, 1:, :])\n",
    "        count_w = self._tensor_size(img[:, :, :, 1:])\n",
    "        h_tv = torch.pow((img[:, :, 1:, :] - img[:, :, :h_x-1, :]), 2).sum()\n",
    "        w_tv = torch.pow((img[:, :, :, 1:] - img[:, :, :, :w_x-1]), 2).sum()\n",
    "        return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "    \n",
    "    def _tensor_size(self, t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "    \n",
    "    def invert(self, target_features: torch.Tensor, \n",
    "               initial_image: Optional[torch.Tensor] = None,\n",
    "               verbose: bool = True,\n",
    "               display_every_n_iterations: Optional[int] = None,\n",
    "               clear_cache_every_n: int = 50) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Invert features to reconstruct an image\n",
    "        \n",
    "        Args:\n",
    "            target_features: Target features to invert\n",
    "            initial_image: Optional initial image (random if None)\n",
    "            verbose: Whether to print progress\n",
    "            display_every_n_iterations: Display current reconstruction every N iterations (None to disable)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'reconstructed_image' and 'loss_history'\n",
    "        \"\"\"\n",
    "        image_size = self.vision_model.get_image_size()\n",
    "        \n",
    "        # Initialize image\n",
    "        if initial_image is None:\n",
    "            # Random initialization\n",
    "            img = torch.randn(1, 3, image_size, image_size, \n",
    "                            device=self.device, requires_grad=True)\n",
    "        else:\n",
    "            img = initial_image.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam([img], lr=self.learning_rate)\n",
    "        \n",
    "        # Store loss history\n",
    "        loss_history = []\n",
    "        \n",
    "        # Setup display if needed\n",
    "        display_output = None\n",
    "        if display_every_n_iterations is not None and display_every_n_iterations > 0:\n",
    "            try:\n",
    "                from IPython.display import display, clear_output\n",
    "                try:\n",
    "                    from ipywidgets import Output\n",
    "                    display_output = Output()\n",
    "                    display(display_output)\n",
    "                except:\n",
    "                    # Fallback: use IPython display directly\n",
    "                    display_output = None\n",
    "            except:\n",
    "                display_every_n_iterations = None  # Disable if IPython not available\n",
    "        \n",
    "        # Optimization loop\n",
    "        for i in range(self.num_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Extract features from current image\n",
    "            # Ensure image is in valid range [0, 1]\n",
    "            img_normalized = torch.sigmoid(img)\n",
    "            current_features = self.vision_model.extract_features(img_normalized)\n",
    "            \n",
    "            # Feature reconstruction loss\n",
    "            feature_loss = nn.functional.mse_loss(current_features, target_features)\n",
    "            \n",
    "            # Total variation loss for regularization\n",
    "            tv_loss = torch.tensor(0.0, device=self.device)\n",
    "            if self.use_tv_loss:\n",
    "                tv_loss = self.total_variation_loss(img_normalized)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = feature_loss + self.tv_weight * tv_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store loss\n",
    "            loss_history.append(total_loss.item())\n",
    "            \n",
    "            # Clear intermediate variables to free memory\n",
    "            del current_features, feature_loss, tv_loss, total_loss\n",
    "            \n",
    "            # Periodically clear CUDA cache\n",
    "            if (i + 1) % clear_cache_every_n == 0:\n",
    "                clear_cuda_cache()\n",
    "            \n",
    "            # Display current reconstruction\n",
    "            if display_every_n_iterations is not None and (i + 1) % display_every_n_iterations == 0:\n",
    "                try:\n",
    "                    from IPython.display import clear_output\n",
    "                    if display_output is not None:\n",
    "                        with display_output:\n",
    "                            clear_output(wait=True)\n",
    "                            current_img = img_normalized.detach()\n",
    "                            img_np = tensor_to_image(current_img)\n",
    "                            \n",
    "                            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "                            \n",
    "                            # Current reconstruction\n",
    "                            axes[0].imshow(img_np)\n",
    "                            axes[0].set_title(f'Iteration {i+1}/{self.num_iterations}')\n",
    "                            axes[0].axis('off')\n",
    "                            \n",
    "                            # Loss curve\n",
    "                            axes[1].plot(loss_history)\n",
    "                            axes[1].set_title('Loss History')\n",
    "                            axes[1].set_xlabel('Iteration')\n",
    "                            axes[1].set_ylabel('Loss')\n",
    "                            axes[1].grid(True)\n",
    "                            axes[1].set_xlim(0, len(loss_history))\n",
    "                            \n",
    "                            plt.tight_layout()\n",
    "                            plt.show()\n",
    "                            \n",
    "                            print(f\"Iteration {i+1}/{self.num_iterations}, Loss: {total_loss.item():.6f}, \"\n",
    "                                  f\"Feature Loss: {feature_loss.item():.6f}, TV Loss: {tv_loss.item():.6f}\")\n",
    "                    else:\n",
    "                        # Fallback: just display without widget\n",
    "                        clear_output(wait=True)\n",
    "                        current_img = img_normalized.detach()\n",
    "                        img_np = tensor_to_image(current_img)\n",
    "                        \n",
    "                        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "                        \n",
    "                        axes[0].imshow(img_np)\n",
    "                        axes[0].set_title(f'Iteration {i+1}/{self.num_iterations}')\n",
    "                        axes[0].axis('off')\n",
    "                        \n",
    "                        axes[1].plot(loss_history)\n",
    "                        axes[1].set_title('Loss History')\n",
    "                        axes[1].set_xlabel('Iteration')\n",
    "                        axes[1].set_ylabel('Loss')\n",
    "                        axes[1].grid(True)\n",
    "                        axes[1].set_xlim(0, len(loss_history))\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        \n",
    "                        print(f\"Iteration {i+1}/{self.num_iterations}, Loss: {total_loss.item():.6f}, \"\n",
    "                              f\"Feature Loss: {feature_loss.item():.6f}, TV Loss: {tv_loss.item():.6f}\")\n",
    "                except:\n",
    "                    # If display fails, just print\n",
    "                    print(f\"Iteration {i+1}/{self.num_iterations}, Loss: {total_loss.item():.6f}, \"\n",
    "                          f\"Feature Loss: {feature_loss.item():.6f}, TV Loss: {tv_loss.item():.6f}\")\n",
    "            \n",
    "            # Print progress (if not displaying)\n",
    "            elif verbose and (i + 1) % 100 == 0:\n",
    "                print(f\"Iteration {i+1}/{self.num_iterations}, Loss: {total_loss.item():.6f}, \"\n",
    "                      f\"Feature Loss: {feature_loss.item():.6f}, TV Loss: {tv_loss.item():.6f}\")\n",
    "        \n",
    "        # Final image\n",
    "        reconstructed = torch.sigmoid(img).detach()\n",
    "        \n",
    "        return {\n",
    "            'reconstructed_image': reconstructed,\n",
    "            'loss_history': loss_history\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b1e70",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image_widget(save_path: str = \"uploaded_image.jpg\") -> str:\n",
    "    \"\"\"\n",
    "    Create an interactive file upload widget for images\n",
    "    \n",
    "    Args:\n",
    "        save_path: Path where the uploaded image will be saved\n",
    "    \n",
    "    Returns:\n",
    "        Path to the saved image file\n",
    "    \"\"\"\n",
    "    if not WIDGETS_AVAILABLE:\n",
    "        print(\"Widgets not available. Please provide image path directly.\")\n",
    "        return None\n",
    "    \n",
    "    upload = FileUpload(\n",
    "        accept='image/*',\n",
    "        multiple=False,\n",
    "        description='Upload Image'\n",
    "    )\n",
    "    \n",
    "    output = Output()\n",
    "    status_label = Label(value=\"Please upload an image file\")\n",
    "    button = Button(description=\"Process Upload\", button_style='success')\n",
    "    \n",
    "    def on_button_click(b):\n",
    "        if len(upload.value) > 0:\n",
    "            with output:\n",
    "                clear_output()\n",
    "                # Get the uploaded file\n",
    "                uploaded_file = list(upload.value.values())[0]\n",
    "                \n",
    "                # Read image data\n",
    "                image_data = uploaded_file['content']\n",
    "                \n",
    "                # Save to file\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    f.write(image_data)\n",
    "                \n",
    "                # Display preview\n",
    "                img = Image.open(io.BytesIO(image_data))\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title('Uploaded Image')\n",
    "                plt.show()\n",
    "                \n",
    "                status_label.value = f\"‚úì Image saved to: {save_path}\"\n",
    "                print(f\"Image successfully uploaded and saved to: {save_path}\")\n",
    "        else:\n",
    "            status_label.value = \"‚ö† Please upload an image first\"\n",
    "    \n",
    "    button.on_click(on_button_click)\n",
    "    \n",
    "    # Display the widget\n",
    "    display(VBox([\n",
    "        Label(value=\"üì§ Image Upload\"),\n",
    "        upload,\n",
    "        button,\n",
    "        status_label,\n",
    "        output\n",
    "    ]))\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def load_and_preprocess_image(image_path: str, image_size: int = 224, device: str = None) -> torch.Tensor:\n",
    "    \"\"\"Load and preprocess an image\"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Resize and convert to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    return img_tensor\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert tensor to numpy image array\"\"\"\n",
    "    # Clamp values to [0, 1]\n",
    "    img = tensor.squeeze(0).cpu().clamp(0, 1)\n",
    "    # Convert to numpy and transpose from CHW to HWC\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return img\n",
    "\n",
    "def visualize_results(original: torch.Tensor, reconstructed: torch.Tensor, \n",
    "                     loss_history: list, model_name: str = \"\"):\n",
    "    \"\"\"Visualize original image, reconstructed image, and loss curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    orig_img = tensor_to_image(original)\n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Reconstructed image\n",
    "    recon_img = tensor_to_image(reconstructed)\n",
    "    axes[1].imshow(recon_img)\n",
    "    axes[1].set_title(f'Reconstructed Image ({model_name})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Loss curve\n",
    "    axes[2].plot(loss_history)\n",
    "    axes[2].set_title('Loss History')\n",
    "    axes[2].set_xlabel('Iteration')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff758cf",
   "metadata": {},
   "source": [
    "## Main Inversion Function\n",
    "\n",
    "Convenience function to perform complete image inversion pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_image_inversion(image_path: str,\n",
    "                           model_type: str = 'dinov2',\n",
    "                           num_iterations: int = 1000,\n",
    "                           learning_rate: float = 0.1,\n",
    "                           use_tv_loss: bool = True,\n",
    "                           tv_weight: float = 0.01,\n",
    "                           visualize: bool = True,\n",
    "                           device: str = None,\n",
    "                           display_every_n_iterations: Optional[int] = None,\n",
    "                           use_keys: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline for image inversion\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        model_type: Type of vision model ('dinov2', 'dinov3', 'clip')\n",
    "        num_iterations: Number of optimization iterations\n",
    "        learning_rate: Learning rate for optimization\n",
    "        use_tv_loss: Whether to use total variation loss\n",
    "        tv_weight: Weight for total variation loss\n",
    "        visualize: Whether to visualize final results\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "        display_every_n_iterations: Display current reconstruction every N iterations (None to disable)\n",
    "        use_keys: For DINOv2, whether to extract keys from last attention layer instead of hidden states\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading vision model: {model_type}\")\n",
    "    if model_type.lower() == 'dinov2' and use_keys:\n",
    "        print(\"Using keys from last attention layer for DINOv2\")\n",
    "    \n",
    "    # Clear cache before loading model\n",
    "    clear_cuda_cache()\n",
    "    \n",
    "    # Load vision model\n",
    "    vision_model = get_vision_model(model_type, device, use_keys=use_keys)\n",
    "    \n",
    "    # Clear cache after loading\n",
    "    clear_cuda_cache()\n",
    "    \n",
    "    # Print memory usage\n",
    "    if device == 'cuda':\n",
    "        allocated, reserved = get_memory_usage()\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB\")\n",
    "    image_size = vision_model.get_image_size()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    print(f\"Loading image from: {image_path}\")\n",
    "    original_image = load_and_preprocess_image(image_path, image_size, device)\n",
    "    \n",
    "    # Extract target features\n",
    "    print(\"Extracting features from original image...\")\n",
    "    with torch.no_grad():\n",
    "        target_features = vision_model.extract_features(original_image)\n",
    "    print(f\"Feature dimension: {target_features.shape}\")\n",
    "    \n",
    "    # Clear cache after feature extraction\n",
    "    clear_cuda_cache()\n",
    "    \n",
    "    # Move target features to CPU if memory is tight (optional)\n",
    "    # Uncomment the next line if you're running out of memory\n",
    "    # target_features = target_features.cpu()\n",
    "    \n",
    "    # Create inverter\n",
    "    inverter = ImageInverter(\n",
    "        vision_model=vision_model,\n",
    "        num_iterations=num_iterations,\n",
    "        learning_rate=learning_rate,\n",
    "        use_tv_loss=use_tv_loss,\n",
    "        tv_weight=tv_weight\n",
    "    )\n",
    "    \n",
    "    # Perform inversion\n",
    "    print(f\"Starting inversion with {num_iterations} iterations...\")\n",
    "    if display_every_n_iterations is not None:\n",
    "        print(f\"Displaying progress every {display_every_n_iterations} iterations\")\n",
    "    results = inverter.invert(target_features, verbose=True, \n",
    "                              display_every_n_iterations=display_every_n_iterations)\n",
    "    \n",
    "    # Visualize final results\n",
    "    if visualize:\n",
    "        visualize_results(original_image, results['reconstructed_image'], \n",
    "                         results['loss_history'], model_type)\n",
    "    \n",
    "    return {\n",
    "        'original_image': original_image,\n",
    "        'reconstructed_image': results['reconstructed_image'],\n",
    "        'loss_history': results['loss_history'],\n",
    "        'target_features': target_features,\n",
    "        'model_type': model_type\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc374a99",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "### Step 1: Upload Your Image\n",
    "\n",
    "First, upload an image using the widget below, or provide a path to an existing image file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload an image using the widget\n",
    "uploaded_image_path = upload_image_widget(\"uploaded_image.jpg\")\n",
    "\n",
    "# Option 2: Use an existing image file\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "\n",
    "# Option 3: Create a sample image for testing (uncomment to use)\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# sample_image = torch.rand(3, 224, 224)\n",
    "# sample_image_np = sample_image.permute(1, 2, 0).numpy()\n",
    "# plt.imsave(\"sample_image.jpg\", sample_image_np)\n",
    "# uploaded_image_path = \"sample_image.jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc1faf",
   "metadata": {},
   "source": [
    "### Example 1: Image Inversion with DINOv2\n",
    "\n",
    "After uploading your image above, run this cell to perform inversion with DINOv2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inversion with DINOv2\n",
    "# Make sure you've uploaded an image in the previous cell or set uploaded_image_path manually\n",
    "\n",
    "# Use uploaded image or specify a path\n",
    "image_path = uploaded_image_path if 'uploaded_image_path' in locals() and uploaded_image_path else \"uploaded_image.jpg\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"‚ö†Ô∏è  Image file '{image_path}' not found.\")\n",
    "    print(\"Please upload an image in the previous cell or provide a valid image path.\")\n",
    "else:\n",
    "    results_dinov2 = perform_image_inversion(\n",
    "        image_path=image_path,\n",
    "        model_type='dinov2',\n",
    "        num_iterations=500,  # Reduced for faster demo\n",
    "        learning_rate=0.1,\n",
    "        visualize=True,\n",
    "        display_every_n_iterations=50  # Display progress every 50 iterations\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef245f",
   "metadata": {},
   "source": [
    "### Example 1b: Image Inversion with DINOv2 using Attention Keys\n",
    "\n",
    "Perform inversion using keys from the last attention layer instead of hidden states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inversion with DINOv2 using attention keys\n",
    "# Use uploaded image or specify a path\n",
    "image_path = uploaded_image_path if 'uploaded_image_path' in locals() and uploaded_image_path else \"uploaded_image.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"‚ö†Ô∏è  Image file '{image_path}' not found.\")\n",
    "    print(\"Please upload an image in the previous cell or provide a valid image path.\")\n",
    "else:\n",
    "    results_dinov2_keys = perform_image_inversion(\n",
    "        image_path=image_path,\n",
    "        model_type='dinov2',\n",
    "        num_iterations=500,\n",
    "        learning_rate=0.1,\n",
    "        visualize=True,\n",
    "        display_every_n_iterations=50,\n",
    "        use_keys=True  # Extract keys from last attention layer instead of hidden states\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a402b",
   "metadata": {},
   "source": [
    "### Example 2: Image Inversion with CLIP\n",
    "\n",
    "Perform inversion with CLIP model using the uploaded image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a006f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inversion with CLIP\n",
    "# Use uploaded image or specify a path\n",
    "image_path = uploaded_image_path if 'uploaded_image_path' in locals() and uploaded_image_path else \"uploaded_image.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"‚ö†Ô∏è  Image file '{image_path}' not found.\")\n",
    "    print(\"Please upload an image in the upload cell above or provide a valid image path.\")\n",
    "else:\n",
    "    results_clip = perform_image_inversion(\n",
    "        image_path=image_path,\n",
    "        model_type='clip',\n",
    "        num_iterations=500,\n",
    "        learning_rate=0.1,\n",
    "        visualize=True,\n",
    "        display_every_n_iterations=50  # Display progress every 50 iterations\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ab2c4",
   "metadata": {},
   "source": [
    "### Example 3: Compare Different Models\n",
    "\n",
    "Compare reconstructions from different models side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76041e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(image_path: str, model_types: list = ['dinov2', 'clip'], \n",
    "                  num_iterations: int = 500,\n",
    "                  display_every_n_iterations: Optional[int] = None):\n",
    "    \"\"\"Compare image inversion results from different models\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load original image\n",
    "    original_image = load_and_preprocess_image(image_path, 224, device)\n",
    "    \n",
    "    results = {}\n",
    "    for model_type in model_types:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing with {model_type}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        results[model_type] = perform_image_inversion(\n",
    "            image_path=image_path,\n",
    "            model_type=model_type,\n",
    "            num_iterations=num_iterations,\n",
    "            visualize=False,\n",
    "            display_every_n_iterations=display_every_n_iterations\n",
    "        )\n",
    "    \n",
    "    # Visualize all results together\n",
    "    num_models = len(model_types)\n",
    "    fig, axes = plt.subplots(1, num_models + 1, figsize=(5 * (num_models + 1), 5))\n",
    "    \n",
    "    # Original\n",
    "    orig_img = tensor_to_image(original_image)\n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Reconstructions\n",
    "    for idx, model_type in enumerate(model_types, 1):\n",
    "        recon_img = tensor_to_image(results[model_type]['reconstructed_image'])\n",
    "        axes[idx].imshow(recon_img)\n",
    "        axes[idx].set_title(f'{model_type.upper()}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models\n",
    "# Use uploaded image or specify a path\n",
    "image_path = uploaded_image_path if 'uploaded_image_path' in locals() and uploaded_image_path else \"uploaded_image.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"‚ö†Ô∏è  Image file '{image_path}' not found.\")\n",
    "    print(\"Please upload an image in the upload cell above or provide a valid image path.\")\n",
    "else:\n",
    "    comparison_results = compare_models(image_path, ['dinov2', 'clip'], num_iterations=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d027d6",
   "metadata": {},
   "source": [
    "## Advanced Usage: Custom Parameters\n",
    "\n",
    "You can fine-tune the inversion process by adjusting various parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example with custom parameters\n",
    "# Use uploaded image or specify a path\n",
    "# image_path = uploaded_image_path if 'uploaded_image_path' in locals() and uploaded_image_path else \"uploaded_image.jpg\"\n",
    "\n",
    "# results = perform_image_inversion(\n",
    "#     image_path=image_path,\n",
    "#     model_type='dinov2',\n",
    "#     num_iterations=2000,      # More iterations for better quality\n",
    "#     learning_rate=0.05,        # Lower learning rate for stability\n",
    "#     use_tv_loss=True,          # Enable TV regularization\n",
    "#     tv_weight=0.02,            # Higher TV weight for smoother results\n",
    "#     visualize=True,\n",
    "#     display_every_n_iterations=100  # Display progress every 100 iterations\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b9b56",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **DINOv2/DINOv3**: Self-supervised vision transformers that learn rich visual representations\n",
    "- **CLIP**: Contrastive language-image pre-trained model that learns aligned image-text representations\n",
    "- **Total Variation Loss**: Regularization term that encourages smoother reconstructions\n",
    "- **Iterations**: More iterations generally lead to better reconstructions but take longer\n",
    "- **Learning Rate**: Lower learning rates are more stable but may require more iterations\n",
    "- **Display During Training**: Use `display_every_n_iterations` parameter to visualize the reconstruction progress in real-time. This shows the current reconstructed image and loss curve, updating every N iterations. Set to `None` to disable.\n",
    "- **DINOv2 Attention Keys**: For DINOv2, you can use `use_keys=True` to extract keys from the last attention layer instead of hidden states. Keys represent what information each token is looking for in attention, which can provide a different representation for inversion. This may yield different reconstruction characteristics compared to using hidden states.\n",
    "- **Memory Management**: The code includes automatic CUDA cache clearing to reduce memory usage. If you still encounter out-of-memory errors, try:\n",
    "  - Using `device='cpu'` instead of GPU (slower but uses less memory)\n",
    "  - Reducing `num_iterations` or `display_every_n_iterations`\n",
    "  - Uncommenting the line that moves target_features to CPU in `perform_image_inversion`\n",
    "  - Using a smaller image size or reducing batch size\n",
    "\n",
    "The inversion process uses gradient-based optimization to find an image that produces features matching the target features extracted from the original image. All tokens (including CLS token) are extracted and normalized using L2 normalization before inversion.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
